# MapReduce and Spark

In previous modules, we learned how to parallelize relational database systems which is useful for optimizing data processing but only works well up to a certain number of machines. Difficulties and headaches come when we start thinking about scaling the relational model on databases split across hundreds or thousands of machines. This became a problem when more and more people, especially every day consumers, started to interact with databases when the Internet exploded, around the turn of the 21st century. Database thinking, models, and technologies needed to catch up to meet the demand. This note will focus one two recent advances in parallel database technologies - MapReduce and Spark (enabled the massive scalability of modern data processing)

## MapReduce

### DFS and High-Level Introduction

Engineers at Google in the early 2000s needed a way to efficiently manage and process the large amounts of data (at petabyte scale and beyond) that was being generated, stored, and indexed by the company. First off, they needed a way to store files across hundreds and eventually thousands of machines since only a few would efinitely not be enough. To do this, they designed a file system in which large files (TBs, PBs) are partitioned into smaller files called chunks (usually 64MB) and then distributed and replicated several times on diffeerent nodes for fault tolerance. This is called a distributed file system (DFS) which has had countless implementations since then from Google's propertietary GFS to Hadoop's open source HDFS. 

Next, they needed a way to efficiently process the large amount of data stored on a DFS. To do this, they built MapReduce which is a high-level programming model and implementation for large-scale parallel data processing. Its name is derived from the two main and separate phases of the process: Map and Reduce. From a high level, we can describe the Map phase as applying a function in parallel to every element of a set of data and the Reduce phase as combining the results of the Map phase into the desired data output. The magic of this paradigm in processing data at scale is it automatically handles the details of issuing and managing tasks in parallel across multiple machines. A user only has to define the Map and Reduce tasks and MapReduce takes care of the rest, similar to how SQL creates an execution plan from a query.

### Data Model and Programming

The data model in MapReduce works on files called bags, which contain pairs. A MapReduce program takes in an input of a bag of pairs and outputs a bag of pairs and outputs a bag of pairs (is optional). The user must provide two stateless functions - and - to define how the input pairs will be transformed into the output pair domain.

The first part of MapReduce - the Map phase - applies a user-provided Map function in parallel to an input of pairs and outputs a bag of pairs. The pairs server as intermediate tuples in the MapReduce process.

```spark
func
```

## Spark


