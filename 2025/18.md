# MapReduce and Spark

In previous modules, we learned how to parallelize relational database systems which is useful for optimizing data processing but only works well up to a certain number of machines. Difficulties and headaches come when we start thinking about scaling the relational model on databases split across hundreds or thousands of machines. This became a problem when more and more people, especially every day consumers, started to interact with databases when the Internet exploded, around the turn of the 21st century. Database thinking, models, and technologies needed to catch up to meet the demand. This note will focus one two recent advances in parallel database technologies - MapReduce and Spark (enabled the massive scalability of modern data processing)

## MapReduce

### DFS and High-Level Introduction

Engineers at Google in the early 2000s needed a way to efficiently manage and process the large amounts of data (at petabyte scale and beyond) that was being generated, stored, and indexed by the company. First off, they needed a way to store files across hundreds and eventually thousands of machines since only a few would efinitely not be enough. To do this, they designed a file system in which large files (TBs, PBs) are partitioned into smaller files called chunks (usually 64MB) and then distributed and replicated several times on diffeerent nodes for fault tolerance. This is called a distributed file system (DFS) which has had countless implementations since then from Google's propertietary GFS to Hadoop's open source HDFS. 

Next, they needed a way to efficiently process the large amount of data stored on a DFS. To do this, they built MapReduce which is a high-level programming model and implementation for large-scale parallel data processing. Its name is derived from the two main and separate phases of the process: Map and Reduce. From a high level, we can describe the Map phase as applying a function in parallel to every element of a set of data and the Reduce phase as combining the results of the Map phase into the desired data output. The magic of this paradigm in processing data at scale is it automatically handles the details of issuing and managing tasks in parallel across multiple machines. A user only has to define the Map and Reduce tasks and MapReduce takes care of the rest, similar to how SQL creates an execution plan from a query.

### Data Model and Programming

The data model in MapReduce works on files called bags, which contain pairs. A MapReduce program takes in an input of a bag of pairs and outputs a bag of pairs and outputs a bag of pairs (is optional). The user must provide two stateless functions - and - to define how the input pairs will be transformed into the output pair domain.

The first part of MapReduce - the Map phase - applies a user-provided Map function in parallel to an input of pairs and outputs a bag of pairs. The pairs server as intermediate tuples in the MapReduce process.

```apache
func map(key1, value1) -> bag((key2, value2))
```

The second part of MapReuce - The Reduce Phase - groups all intermediate pairs outputted by the Map phase with the same and processes each group into a single bag of output values defined by a user-provided Reduce function. This function is also run in parallel with a machine or worker thread processing a single group of pairs with the same. The pairs in this function serve as the output tuples in the MapReduce process.

```
func reduce(key2, bag(value2)) -> bag((key3, value3))
```

### Word Count Example

Let's start with an example of counting the number of occurences of each word in a large collection of documents. Each document is a bag of key-value pairs with key being the document and value being the entire set of words in that document. We define the following map function to emit an intermediate pair for every word that appears in a document. Note that this function is stateless so we can run it in parallel.

```apache
map(String key, String value):
  // key: document name
  // value: document contents
  for each word w in value:
    emitIntermediate(w, 1)
```

After applying this map function to all documents, we essentially have a collection of all individual words that appear in all documents wiht  a1 assigned to each to represent the number of appeareances. For example:

```shell
(apple, 1)
(banana, 1)
(banana, 1)
(book, 1)
...
(shoe, 1)
```

MapReduce then groups intermediate pairs with the same word key and for each group, combined their value (in this case all 1s) into an iterator. The following reduce function takes the iterator for each word key and returns the sum over the 1s in the values, resulting in the count of each word across all documents.

```
reduce(String key, Iterator values):
  // key: a word
  // values: a list of counts
  int sum = 0;
  for each v in values:
    sum += v
  emit(key, sum)
```

Here is a visual diagram of what happens at a high level in MapReduce. First, we map each document to a list of (word, 1) pairs. Then, we shuffle (or group) these intermediate pairs based on the word and create a list of 1s for each group. Finally, the reduce function sums over this list to generate the word count for each word.

![image](https://github.com/user-attachments/assets/000f220d-3932-4b29-b475-634771667aaa)

### Workers

Map reduce processes are split up and run by **workers** which are processes that execute one task at a time. With 1 worker per core, we can have multiple workers per node in our system. Here is the same Word Count example but with worker tasks denoted by what part of the MapReduce process they handle. Both the Map and Reduce phases can be split among different workers on different machines, with workers performing independent tasks in parallel. Also the shuffling process here is automatically handled by the system.

My guess is did is document id and v is the contents.

![image](https://github.com/user-attachments/assets/9a686ff0-4ccb-42c4-93c8-61d867ccab81)

### Implementation
Like 2PC, MapReduce designates one machine as the leader node. Its role is to accept new MapReduce tasks and assign them to workers later on. The leader begins by partioning the input file into M splits by key and assigning workers to M map tasks, keeping track of progress as workers perform their tasks. Workers write their output to the local disk and partition their output into R regions. The leader then assigns workers to the R reduce tasks which then write the final output to disk once complete.

### Fault Tolerance

Now, we can start thinking of possibilities of failure in this implementation. One of the most common faults is the faulure of a worker performing a map task. One way to solve this is to just restart a new mapper task. One way to solve this is to just restart a new mapper task and read the data it is supposed to process from the disk again. However, since the Reduce phase must wait until the entire Map phase completes, does this mean if one map task fails then we must restart the entire Map phase again?

No. If we write the intermediate result of the Map phase to disk, then we can avoid this. This means that the Reduce phase workers will have to read files from disk as its input and exactly like the Map phase workers - if one crashes during execution, it just gets restarted.

Another interesting detail about hte MapReduce implementation is how it handles machines that take an unusually long time to complete one of its tasks, also known as a straggler. These stragglers could be the result of faulty hardware, too many tasks assigned to a machine, or other reasons. To avoid this, MapReduce preemptively starts new executions of the last few remaining in-progress tasks with the hope that all the necessary tasks are completed.

![image](https://github.com/user-attachments/assets/ed99745e-e187-4cb2-bee2-86613a2a8004)

For example, in this case of 3 workers, worker 1 and 2 complete their tasks first while worker 3 straggles. MapReduce then decides to utilize worker 1 and 2, which are now idle, to start the same task as worker 3 as backup. Once worker 2 finishes this task, the remaining tasks on worker 1 and 3 are killed and the MapReduce process can proceed.

### Selection, Group By, Join

Let's look at how to implement a selection (from relational algebra) using MapReduce. If we want to perform a query like <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>&#x3C3;</mi>
    <mrow data-mjx-texclass="ORD">
      <mi>A</mi>
      <mo>=</mo>
      <mn>123</mn>
    </mrow>
  </msub>
  <mrow data-mjx-texclass="ORD">
    <mo stretchy="false">(</mo>
    <mi>R</mi>
    <mo stretchy="false">)</mo>
  </mrow>
</math>



## Spark




